---
title: "Student Performance Predictions for Online Learning"
author: "Makayla Whitney"
author: "Github: https://github.com/makwhit6/654finalproject "
fontsize: 12
output: pdf_document
---

```{r setup, include=FALSE, warning = FALSE}
knitr::opts_chunk$set(echo = TRUE)
require(caret)
require(recipes)
require(finalfit)
require(glmnet)
library(dplyr)
library(ggplot2)
library(corrplot)
```

```{r load data, include = FALSE, warning = FALSE}
studentperform <- read.csv("C:/Users/makay/OneDrive/Desktop/Data Science/Machine Learning/654finalproject/data/studentdata.csv",header=TRUE)

#add a column for ID
stuperform <- mutate(studentperform, id = row_number())

#modify Class column to represent numeric values rather than characters
stuperform[stuperform == "L"] <- 0
stuperform[stuperform == "M"] <- 5
stuperform[stuperform == "H"] <- 10

stuperform$Class <- as.numeric(stuperform$Class)

str(stuperform)

```

|   Online learning has been a tool used for several years now; however, the coronavirus pandemic kick-started advancements in online technology and educational tools allowing students to continue their education from anywhere. This project explores student performance data taken from an online education management program. Student demographics as well as grade level, attendance rates, and other educational attributes are used to predict which category (low, medium, high final grade) students will fall into. The outcome of interest is which variable has the greatest effect on overall student performance. I predict overall successful performance will either be contingent upon the subject matter or student attendance. The subject matter has an effect on student mood and performance in school. Attendance is another factor that effects student’s interest in the subject as well as their overall performance. 
|   The potential benefits for this type of model are to see where there are pitfalls in an online program. By using factors outside of student demographics, an online program would be able to observe what is effecting their student’s performance the most. Outliers or variables with significant negative effects can be brought forth to program developers for areas of improvement. This type of model could also be applied to in-person schools to observe any variables having a negative effect on overall student performance. 

```{r recipe, warning = FALSE, message = FALSE, echo=FALSE}

outcome <- c('Class')

id <- c('id')

categorical <- c('Gender',
                 'Nationality',
                 'PlaceofBirth',
                 'StageID',
                 'GradeID', 
                 'SectionID',
                 'Topic',
                 'Semester',
                 'Relation',
                 'ParentAnsweringSurvey',
                 'ParentschoolSatisfaction',
                 'StudentAbsenceDays')

numeric <- c('raisedhands',
              'VisITedResources',
              'AnnouncementsView',
              'Discussion')

for(i in categorical){
  stuperform[,i] <- as.factor(stuperform[,i])
}


student_blueprint <- recipe(x = stuperform,
                         vars = c(outcome, id, categorical, numeric),
                         roles = c('outcome', 'id', rep('predictor', 16))) %>%
  step_indicate_na(all_of(categorical), all_of(numeric)) %>%
  step_zv(all_numeric()) %>%
  step_impute_mean(all_of(numeric)) %>%
  step_impute_mode(all_of(categorical)) %>%
  step_ns(all_of(numeric), deg_free = 3) %>%
  step_normalize(paste0(numeric, '_ns_1'),
                 paste0(numeric, '_ns_2'),
                 paste0(numeric, '_ns_3')) %>%
  step_dummy(all_of(categorical), one_hot = TRUE)

student_blueprint

prepare <- prep(student_blueprint, 
                training = stuperform)

baked_student <- bake(prepare, new_data = stuperform)

```

##Data
|   The original data set is housed on [Kaggle] (https://www.kaggle.com/aljarah/xAPI-Edu-Data) and was modified for the purposes of this project. This educational data set was collected from Kalboard 360, a subscription-based, online learning management system that offers students access to synchronous classes from any device with an internet connection. This data set was collected using a learner activity tracker tool called experience API (xAPI). “The xAPI is a component of the training and learning architecture (TLA) that enables to monitor learning progress and learner’s actions like reading an article or watching a training video. The experience API helps the learning activity providers to determine the learner, activity and objects that describe a learning experience” (Aljarah, I., 2016). 
This data set contains records from 480 students and 16 features as displayed in Table 1. The original author of the data set categorized these features into three main categories:
|       1.	Demographic features (gender, nationality, etc.)
|       2.	Academic background features (educational attainment, grade level, etc.)
|       3.	Behavioral features (amount of times a student raises their hand or accesses resources)

**Table 1.** _Variable Descriptions_
```{r table1, warning = FALSE, message = FALSE, echo=FALSE}
gender <- data.frame(Variable = c("Gender"),
                     Explanation = c("student's gender"),
                     Options = c("Male or Female"))
nation <- data.frame(Variable = c("Nationality"),
                     Explanation = c("student's nationality"),
                     Options = c("Kuwait, Lebanon, Egypt, Saudi Arabia, USA, Jordan, Venezuela, Iran,    
                                 Tunis, Morocco, Syria, Palestine, Iraq, Lybia"))
birth <- data.frame(Variable = c("PlaceofBirth"),
                     Explanation = c("student's country of birth"),
                     Options = c("Kuwait, Lebanon, Egypt, Saudi Arabia, USA, Jordan, Venezuela, Iran,    
                                 Tunis, Morocco, Syria, Palestine, Iraq, Lybia"))
edstage <- data.frame(Variable = c("StageID"),
                     Explanation = c("student's current educational level"),
                     Options = c("Lower Level, Middle School, High School"))
grade <- data.frame(Variable = c("GradeID"),
                     Explanation = c("student's current grade level"),
                     Options = c("G-01, G-02, G-03, G-04, G-05, G-06, G-07, G-08, G-09, G-10, G-11, G-12"))
section <- data.frame(Variable = c("SectionID"),
                     Explanation = c("student's current classroom"),
                     Options = c("A, B, C"))
topic <- data.frame(Variable = c("Topic"),
                     Explanation = c("course topics available to students"),
                     Options = c("English, Spanish, French, Arabic, IT, Math, Chemistry, Biology, Science, 
                                 History, Quran, Geology"))
semester <- data.frame(Variable = c("Semester"),
                     Explanation = c("semester of current school year"),
                     Options = c("First, Second"))
parent <- data.frame(Variable = c("Relation"),
                     Explanation = c("Parent responsible for student"),
                     Options = c("Mom, father"))
hands <- data.frame(Variable = c("raisedhands"),
                     Explanation = c("amount of times a student raises their hand during class"),
                     Options = c("1-100"))
resources <- data.frame(Variable = c("VisitedResources"),
                     Explanation = c("amount of times a student accesses course content"),
                     Options = c("1-100"))
announce <- data.frame(Variable = c("AnnouncementsView"),
                     Explanation = c("amount of times a student checks new announcements"),
                     Options = c("1-100"))
discuss <- data.frame(Variable = c("Discussion"),
                     Explanation = c("amount of times a student participates in discussion groups"),
                     Options = c("1-100"))
survey <- data.frame(Variable = c("ParentAnsweringSurvey"),
                     Explanation = c("amount of surveys provided by the school answered by parents"),
                     Options = c("1-100"))
satisfy <- data.frame(Variable = c("ParentschoolSatisfaction"),
                     Explanation = c("amount of parent satisfaction"),
                     Options = c("1-100"))
absent <- data.frame(Variable = c("StudentAbsenceDays"),
                     Explanation = c("amount of days each student was absent during the school year"),
                     Options = c("above-7, under-7"))
class <- data.frame(Variable = c("Class"),
                     Explanation = c("students classified into numerical intervals according to their total
                                     grade"),
                     Options = c("Low-Level(0): 0 to 69, Middle-Level(5): 70 to 89, High-Level(10): 
                                 90-100."))
id <- data.frame(Variable = c("ID"),
                     Explanation = c("numeric identification for individual students"),
                     Options = c("1-480"))

var_descrip <- rbind(gender, nation, birth, edstage, grade, section, topic, semester, parent, hands, resources, announce, discuss, survey, satisfy, absent, class, id)
var_descrip

```

|   Of the 480 students, 305 are male and 175 are female. Students’ nationalities breakdown into the following: 179 from Kuwait, 172 from Jordan, 28 from Palestine, 22 from Iraq, 17 from Lebanon, 12 from Tunis, 11 from Saudi Arabia, 9 from Egypt, 7 from Syria, 6 from USA, Iran and Libya, 4 from Morocco and one from Venezuela. Data was collected over the course of two semesters with 245 records from the first semester and 235 records from the second. It is interesting to note that a higher proportion of students missed less than 7 days of school during the semester. Finally, the data set contains information gathered from parent surveys and parental level of school satisfaction. Overall, 270 parents answered the surveys and 292 parents noted their satisfaction with the school. 

##Models
|   I first chose a model using linear regression without any regularization; however, this was not chosen as it produced model fitting errors. Instead a model with 10-fold cross-validation using ridge regression was chosen to predict the class area students fell into. In order to find the optimal lambda value for evaluation, a k-fold cross-validation and plot were produced. Figure 1 displays the lambda score of 0.3673767 to be the best fit for all of the models. A model with 10-fold cross-validation using lasso regression was chosen as a comparison to the ridge regression model. Again, the optimal lambda score was used to produce the most effective prediction. Finally, a bagged tree model using a random forest was developed to predict the probability of student performance. Each of these models were chosen to determine the extent to which increasing the model complexity impacted the accuracy of the predictions. The mean-absolute error (MAE), root-mean squared error (RMSE), and the R-squared (Rsq) scores were calculated to compare the performance of each model. 

```{r data prep, warning = FALSE, message = FALSE, echo=FALSE}
#evaluate for missing variables
missing <- ff_glimpse(stuperform)$Continuous

head(missing)
#no missing variables present to remove

#set the seed. create test and traing dataset where training data is 80% and test is 20%
set.seed(12092021)
  
loc      <- sample(1:nrow(stuperform), round(nrow(stuperform) * 0.8))
student_train  <- stuperform[loc, ]
student_test  <- stuperform[-loc, ]

```

```{r model 1, warning = FALSE, message = FALSE, echo=FALSE}

# Randomly shuffle the data

student_train = student_train[sample(nrow(student_train)),]

# Create 10 folds with equal size

folds = cut(seq(1,nrow(student_train)),breaks=10,labels=FALSE)

# Create the list for each fold 

my.indices <- vector('list',10)
for(i in 1:10){
  my.indices[[i]] <- which(folds!=i)
}

cv <- trainControl(method = "cv",
                   index  = my.indices)

#determine the optimal lambda
#define response variable
y <- stuperform$Class

#define matrix of predictor variables
x <- data.matrix(stuperform[, c('Gender',
                 'Nationality',
                 'PlaceofBirth',
                 'StageID',
                 'GradeID', 
                 'SectionID',
                 'Topic',
                 'Semester',
                 'Relation',
                 'ParentAnsweringSurvey',
                 'ParentschoolSatisfaction',
                 'StudentAbsenceDays',
                 'raisedhands',
              'VisITedResources',
              'AnnouncementsView',
              'Discussion')])
```

```{r fig1, warning = FALSE, message = FALSE, echo=FALSE, fig.align = 'center'}
#perform k-fold cross-validation to find optimal lambda value
cv_model <- cv.glmnet(x, y, alpha = 0)

#find optimal lambda value that minimizes test MSE
best_lambda <- cv_model$lambda.min
best_lambda
#0.3673767

#produce plot of test MSE by lambda value
plot(cv_model)
```
**Figure 1.** _Optimal Lambda Value_

```{r model 1 cont, warning = FALSE, message = FALSE, echo=FALSE}
#create a grid
grid <- data.frame(alpha = 0, lambda = 0.3673767) 
grid


# Train the model

ridge <- caret::train(student_blueprint, 
                      data      = student_train, 
                      method    = "glmnet", 
                      trControl = cv,
                      tuneGrid  = grid)

ridge$bestTune

predict_te_ridge <- predict(ridge, student_test)

r_rsq_te <- cor(student_test$Class,predict_te_ridge)^2
r_rsq_te
# 0.6714214
r_mae_te <- mean(abs(student_test$Class - predict_te_ridge))
r_mae_te
# 1.707019
r_rmse_te <- sqrt(mean((student_test$Class - predict_te_ridge)^2))
r_rmse_te
# 2.093097

```

```{r model 2, warning = FALSE, message = FALSE, echo=FALSE}
grid2 <- data.frame(alpha = 1, lambda = 0.3673767) 

grid2

# Train the model

lasso <- caret::train(student_blueprint, 
                       data      = student_train, 
                       method    = "glmnet", 
                       trControl = cv,
                       tuneGrid  = grid2)

lasso$bestTune

#getting errors in this section
predict_te_lasso<- predict(lasso, student_test)

l_rsq_te <- cor(student_test$Class,predict_te_lasso)^2
l_rsq_te
#0.6529293
l_mae_te <- mean(abs(student_test$Class - predict_te_lasso))
l_mae_te
# 1.886536
l_rmse_te <- sqrt(mean((student_test$Class - predict_te_lasso)^2))
l_rmse_te
# 2.135996

```
 
```{r model 3, warning = FALSE, message = FALSE, echo=FALSE}

set.seed(12092021)

# Cross validation settings 
student_train = student_train[sample(nrow(student_train)),]

folds = cut(seq(1,nrow(student_train)),breaks=10,labels=FALSE)
     
my.indices <- vector('list',10)
    for(i in 1:10){
      my.indices[[i]] <- which(folds!=i)
    }
    
cv <- trainControl(method = "cv",
                       index  = my.indices)

rgrid <- expand.grid(mtry = 16,splitrule='variance',min.node.size=2)
#rgrid

student_rforest <- caret::train(student_blueprint,
                        data      = student_train,
                        method    = 'ranger',
                        trControl = cv,
                        tuneGrid  = rgrid,
                        num.trees = 100,
                        max.depth = 10)
 
student_rforest$times

#checking random forest model on the test dataset
rpredict_te <- predict(student_rforest, student_test)

# RMSE for random forest model
ran_rmse <- sqrt(mean((student_test$Class - rpredict_te)^2))
ran_rmse
#2.061287

#calculate MAE for random tree model
test_predict <- predict(student_rforest, student_test)

#MAE
ran_mae <- mean(abs(student_test$Class - test_predict))
ran_mae
#1.645688

#calculate rsqd for bagged tree model
ran_rsqd <- cor(student_test$Class, test_predict)^2
ran_rsqd
#0.668946

```

##Model fit
|   Table 2 provides a cross-comparison of models using their MAE, RMSE, and Rsq scores. The RMSE score for the bagged tree model using a random forest was the lowest out of the three models. Its RMSE score of 2.044026 describes the standard deviation of the differences between the predicted and observed values. The random forest model produced an R-squared score of 0.6727365, so about 67% of the model’s predictions are correct. This R-squared score was the highest out of the three models, which displays it is the best at predicting student performance. This model could be improved to produce a higher R-squared score by scaling features and continuing to tune hyperparameters. 

**Table 2.** _Model Comparison_
```{r model comparison, warning = FALSE, message = FALSE, echo=FALSE}
ridgereg <- data.frame(Model = c("Linear Regression with Ridge Penalty"),
                    RMSE = c(r_rmse_te),
                     MAE = c(r_mae_te),
                     Rsq = c(r_rsq_te))

lassoreg <- data.frame(Model = c("Linear Regression with Lasso Penalty"),
                    RMSE = c(l_rmse_te),
                     MAE = c(l_mae_te),
                     Rsq = c(l_rsq_te))

ranmod <- data.frame(Model = c("Random Forest Model"),
                    RMSE = c(ran_rmse),
                     MAE = c(ran_mae),
                     Rsq = c(ran_rsqd))

#Final Table
ModEvalTable <- rbind(ridgereg, lassoreg, ranmod)
ModEvalTable

#We want the lowest RMSE score
```

##Data Visualizations
|   During initial data exploration, a histogram was created to check whether the dependent variable (Class) followed a normal distribution pattern. Figure 2 displays the highest amount of students observed falls in the middle rather than on either side indicating a normal distribution pattern.
```{r fig 1, warning = FALSE, message = FALSE, echo=FALSE}
hist(stuperform$Class)

```
**Figure 2.** _Distribution Pattern of Class Variable_

|   A correlation matrix was also developed to determine which variable was the most important in predicting the outcome. A student’s performance and grade (Class) was most highly correlated with the amount of times they raised their hand during instruction. 
```{r fig 2, warning = FALSE, message = FALSE, echo=FALSE}
student.cor = cor(baked_student)
correlation <- corrplot(student.cor)

```
**Figure 3.** _Student Performance Correlation Matrix_

##Discussion
|   Within this project, I tested five models before choosing the best three. Both the model using linear regression without any regularization and bagged tree model did not produce favorable outcomes. All three models performed to similar standards with no significant gaps in performance. Each model produced MAE, RMSE, and R-squared scores that were decimals away from one another. From the three models observed, they all predicted the outcome with over 65% accuracy. This percent would need to be improved if the model were to be used to collect data used to improve online schools. As noted above, this could come from scaling features and continuing to tune hyperparameters or it may be a matter of choosing a different model type. 
|   The amount of times a student raised their hand during instruction had the highest correlation to student performance overall. This was surprising to me as my hypothesis was that the class subject or student attendance would have the highest correlation. However, it shows that this online program does an excellent job of engaging their students in the material. An engaging classroom would hold students’ attention and create peer collaboration fostering deeper learning which, in turn, would lead to higher student success. This online program should be praised for its commitment to student participation. While the model’s prediction accuracy does need to be improved, there is promise for practical application in the future. A model that uses student learning characteristics to predict their overall performance could be used several times through the school year to continuously be evaluating areas of improvement. 

##Conclusion
|   Due to the pandemic, online classes and schools have become a more commonly utilized tool. Individuals are able to receive an education at any stage in life and anywhere geographically around the globe on their time schedule. More individuals are prone to obtain a degree online due to the drastic advancements in educational technology. If this trend is to continue, an evaluation metric will need to be developed in order to accommodate online students. A prediction model, like the one developed for this project, is a sustainable and simple way to evaluate online programs based on predictions of student success. 